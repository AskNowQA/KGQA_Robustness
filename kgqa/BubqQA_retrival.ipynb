{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:23.169246Z",
     "start_time": "2019-08-12T09:16:23.028848Z"
    }
   },
   "outputs": [],
   "source": [
    "! touch BuboQA/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:29:06.524791Z",
     "start_time": "2019-08-12T10:29:06.521236Z"
    }
   },
   "outputs": [],
   "source": [
    "# load the model\n",
    "import sys\n",
    "sys.path.append('./BuboQA/entity_detection/nn')\n",
    "sys.path.append('./BuboQA/entity_linking/')\n",
    "sys.path.append('./BuboQA/relation_prediction/nn')\n",
    "sys.path.append('./BuboQA/evidence_integration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:23.179328Z",
     "start_time": "2019-08-12T09:16:23.175642Z"
    }
   },
   "outputs": [],
   "source": [
    "from BuboQA.entity_detection.nn.args import get_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:46:23.897046Z",
     "start_time": "2019-08-12T10:46:23.878601Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "from torchtext import data\n",
    "from BuboQA.entity_detection.nn.args import get_args\n",
    "from BuboQA.entity_detection.nn.evaluation import evaluation\n",
    "from BuboQA.entity_detection.nn.sq_entity_dataset import SQdataset\n",
    "from BuboQA.relation_prediction.nn.sq_relation_dataset import SQdataset as relSQdataset\n",
    "from BuboQA.entity_linking import entity_linking\n",
    "from BuboQA.relation_prediction.nn import relation_prediction as rel_rp\n",
    "from BuboQA.evidence_integration import evidence_integration as ei\n",
    "from collections import defaultdict\n",
    "from BuboQA.evidence_integration.util import clean_uri, processed_text, www2fb, rdf2fb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:24.960818Z",
     "start_time": "2019-08-12T09:16:24.956917Z"
    }
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:49:37.785469Z",
     "start_time": "2019-08-12T10:49:37.771879Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# args = get_args()\n",
    "class Args:\n",
    "    seed = 42\n",
    "    cuda = True\n",
    "    data_dir = './BuboQA/data/processed_simplequestions_dataset'\n",
    "    results_path = './BuboQA/entity_detection/nn/query_text'\n",
    "    rel_results_path = './BuboQA/results'\n",
    "    trained_model = './BuboQA/entity_detection/nn/saved_checkpoints/lstm/id1_best_model.pt'\n",
    "    rel_trained_model = './BuboQA/relation_prediction/nn/saved_checkpoints/cnn/id1_best_model.pt'\n",
    "    batch_size = 32\n",
    "    gpu = 0\n",
    "    entity_detection_mode = 'LSTM'\n",
    "    dataset = 'EntityDetection'\n",
    "    model_type = 'lstm'\n",
    "    index_ent = 'BuboQA/indexes/entity_2M.pkl'\n",
    "    query_dir = 'BuboQA/entity_detection/nn/query_text/lstm'\n",
    "    hits = 100\n",
    "    output_dir = 'BuboQA/entity_linking/results'\n",
    "    relation_prediction_mode = 'CNN'\n",
    "    rel_dataset = 'RelationPrediction'\n",
    "    ent_type = 'lstm'\n",
    "    rel_type = 'cnn'\n",
    "    index_reachpath = 'BuboQA/indexes/reachability_2M.pkl'\n",
    "    index_degreespath = 'BuboQA/indexes/degrees_2M.pkl'\n",
    "    data_path = 'BuboQA/data/processed_simplequestions_dataset/amt_test.txt'\n",
    "    ent_path = 'BuboQA/entity_linking/results/lstm/test-h100.txt'\n",
    "    rel_path = 'BuboQA/relation_prediction/nn/results/cnn/test.txt'\n",
    "    wiki_path = 'BuboQA/data/fb2w.nt'\n",
    "    hits_ent = 50\n",
    "    hits_rel = 5\n",
    "    heuristics = True\n",
    "    output_dir = 'BuboQA/evidence_integration/results'\n",
    "    \n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:24.985400Z",
     "start_time": "2019-08-12T09:16:24.975892Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "random.seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:25.041410Z",
     "start_time": "2019-08-12T09:16:24.990055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: You are using GPU for training\n"
     ]
    }
   ],
   "source": [
    "if not args.cuda:\n",
    "    args.gpu = -1\n",
    "if torch.cuda.is_available() and args.cuda:\n",
    "    print(\"Note: You are using GPU for training\")\n",
    "    # torch.cuda.set_device(args.gpu)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "    print(\"Warning: You have Cuda but not use it. You are using CPU for training.\")\n",
    "\n",
    "TEXT = data.Field(lower=True)\n",
    "ED = data.Field()\n",
    "RELATION = data.Field(sequential=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:29.583927Z",
     "start_time": "2019-08-12T09:16:25.046131Z"
    }
   },
   "outputs": [],
   "source": [
    "TEXT = pickle.load(open('BuboQA/entity_detection/nn/TEXT.pickle', 'rb'))\n",
    "ED = pickle.load(open('BuboQA/entity_detection/nn/ED.pickle','rb'))\n",
    "train, dev, test = SQdataset.splits(TEXT, ED, path=args.data_dir, test='amt_test.txt')\n",
    "RELATION.build_vocab(train, dev) # This can also be loaded via pickle. But for now keeping it static."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:29.592035Z",
     "start_time": "2019-08-12T09:16:29.585509Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load old torchtext things.\n",
    "\n",
    "# TEXT.build_vocab(train, dev, test)\n",
    "# ED.build_vocab(train, dev, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:29.597486Z",
     "start_time": "2019-08-12T09:16:29.593273Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iter = data.Iterator(train, batch_size=args.batch_size, device=\"cuda\", train=True, repeat=False,\n",
    "                                   sort=False, shuffle=True)\n",
    "dev_iter = data.Iterator(dev, batch_size=args.batch_size, device=\"cuda\", train=False, repeat=False,\n",
    "                                   sort=False, shuffle=False)\n",
    "test_iter = data.Iterator(test, batch_size=args.batch_size, device=\"cuda\", train=False, repeat=False,\n",
    "                                   sort=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:36.809419Z",
     "start_time": "2019-08-12T09:16:29.599146Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EntityDetection(\n",
      "  (embed): Embedding(61332, 300)\n",
      "  (lstm): LSTM(300, 300, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.3)\n",
      "  (relu): ReLU()\n",
      "  (hidden2tag): Sequential(\n",
      "    (0): Linear(in_features=600, out_features=600, bias=True)\n",
      "    (1): BatchNorm1d(600, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): Dropout(p=0.3)\n",
      "    (4): Linear(in_features=600, out_features=7, bias=True)\n",
      "  )\n",
      ")\n",
      "RelationPrediction(\n",
      "  (embed): Embedding(61332, 300)\n",
      "  (conv1): Conv2d(1, 300, kernel_size=(2, 300), stride=(1, 1), padding=(1, 0))\n",
      "  (conv2): Conv2d(1, 300, kernel_size=(3, 300), stride=(1, 1), padding=(2, 0))\n",
      "  (conv3): Conv2d(1, 300, kernel_size=(4, 300), stride=(1, 1), padding=(3, 0))\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (fc1): Linear(in_features=900, out_features=1698, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.load(args.trained_model, map_location=lambda storage,location: storage.cuda(args.gpu))\n",
    "rel_model = torch.load(args.rel_trained_model, map_location=lambda storage,location: storage.cuda(args.gpu))\n",
    "print(model)\n",
    "print(rel_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:36.918284Z",
     "start_time": "2019-08-12T09:16:36.827593Z"
    }
   },
   "outputs": [],
   "source": [
    "if args.dataset == 'EntityDetection':\n",
    "    index2tag = np.array(ED.vocab.itos)\n",
    "else:\n",
    "    print(\"Wrong Dataset\")\n",
    "    exit(1)\n",
    "\n",
    "index2word = np.array(TEXT.vocab.itos)\n",
    "index2tag = np.array(ED.vocab.itos)\n",
    "\n",
    "results_path = os.path.join(args.results_path, args.entity_detection_mode.lower())\n",
    "if not os.path.exists(results_path):\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "\n",
    "rel_results_path = os.path.join(args.rel_results_path, args.relation_prediction_mode.lower())\n",
    "if not os.path.exists(rel_results_path):\n",
    "    os.makedirs(rel_results_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:36.953522Z",
     "start_time": "2019-08-12T09:16:36.923775Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert(fileName, idFile, outputFile):\n",
    "    fin = open(fileName)\n",
    "    fid = open(idFile)\n",
    "    fout = open(outputFile, \"w\")\n",
    "\n",
    "    for line, line_id in tqdm(zip(fin.readlines(), fid.readlines())):\n",
    "        query_list = []\n",
    "        query_text = []\n",
    "        line = line.strip().split('\\t')\n",
    "        sent = line[0].strip().split()\n",
    "        pred = line[1].strip().split()\n",
    "        for token, label in zip(sent, pred):\n",
    "            if label == 'I':\n",
    "                query_text.append(token)\n",
    "            if label == 'O':\n",
    "                query_text = list(filter(lambda x: x != '<pad>', query_text))\n",
    "                if len(query_text) != 0:\n",
    "                    query_list.append(\" \".join(list(filter(lambda x:x!='<pad>', query_text))))\n",
    "                    query_text = []\n",
    "        query_text = list(filter(lambda x: x != '<pad>', query_text))\n",
    "        if len(query_text) != 0:\n",
    "            query_list.append(\" \".join(list(filter(lambda x:x!='<pad>', query_text))))\n",
    "            query_text = []\n",
    "        if len(query_list) == 0:\n",
    "            query_list.append(\" \".join(list(filter(lambda x:x!='<pad>',sent))))\n",
    "        fout.write(\" %%%% \".join([line_id.strip()]+query_list)+\"\\n\")\n",
    "\n",
    "\n",
    "def predict(dataset_iter=test_iter, dataset=test, data_name=\"test\"):\n",
    "    print(\"Dataset: {}\".format(data_name))\n",
    "    model.eval()\n",
    "    dataset_iter.init_epoch()\n",
    "\n",
    "    n_correct = 0\n",
    "    fname = \"{}.txt\".format(data_name)\n",
    "    temp_file = 'tmp'+fname\n",
    "    results_file = open(temp_file, 'w')\n",
    "\n",
    "    gold_list = []\n",
    "    pred_list = []\n",
    "\n",
    "    for data_batch_idx, data_batch in enumerate(dataset_iter):\n",
    "        scores = model(data_batch)\n",
    "        if args.dataset == 'EntityDetection':\n",
    "            n_correct += torch.sum(torch.sum(torch.max(scores, 1)[1].view(data_batch.ed.size()).data == data_batch.ed.data, dim=1) \\\n",
    "                              == data_batch.ed.size()[0]).item()\n",
    "            index_tag = np.transpose(torch.max(scores, 1)[1].view(data_batch.ed.size()).cpu().data.numpy())\n",
    "            tag_array = index2tag[index_tag]\n",
    "            index_question = np.transpose(data_batch.text.cpu().data.numpy())\n",
    "            question_array = index2word[index_question]\n",
    "            gold_list.append(np.transpose(data_batch.ed.cpu().data.numpy()))\n",
    "            gold_array = index2tag[np.transpose(data_batch.ed.cpu().data.numpy())]\n",
    "            pred_list.append(index_tag)\n",
    "            for  question, label, gold in zip(question_array, tag_array, gold_array):\n",
    "                results_file.write(\"{}\\t{}\\t{}\\n\".format(\" \".join(question), \" \".join(label), \" \".join(gold)))\n",
    "        else:\n",
    "            print(\"Wrong Dataset\")\n",
    "            exit()\n",
    "\n",
    "    if args.dataset == 'EntityDetection':\n",
    "        P, R, F = evaluation(gold_list, pred_list, index2tag, type=False)\n",
    "        print(\"{} Precision: {:10.6f}% Recall: {:10.6f}% F1 Score: {:10.6f}%\".format(\"Dev\", 100. * P, 100. * R,\n",
    "                                                                                     100. * F))\n",
    "    else:\n",
    "        print(\"Wrong dataset\")\n",
    "        exit()\n",
    "    results_file.flush()\n",
    "    results_file.close()\n",
    "    convert(temp_file, os.path.join(args.data_dir, \"lineids_{}.txt\".format(data_name)), os.path.join(results_path,\"query.{}\".format(data_name)))\n",
    "    os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running the entity predicition module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:16:43.472150Z",
     "start_time": "2019-08-12T09:16:36.958657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: amt_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./BuboQA/entity_detection/nn/entity_detection.py:58: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n",
      "  outputs, (ht, ct) = self.lstm(x)\n",
      "4487it [00:00, 39712.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev Precision:  92.333456% Recall:  93.029168% F1 Score:  92.680006%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21687it [00:00, 68986.60it/s]\n"
     ]
    }
   ],
   "source": [
    "# run the model on the test set and write the output to a file\n",
    "predict(dataset_iter=test_iter, dataset=test, data_name=\"amt_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T11:37:26.829591Z",
     "start_time": "2019-08-07T11:37:26.825049Z"
    }
   },
   "source": [
    "#### Running the entity Linking module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:18:15.370199Z",
     "start_time": "2019-08-12T09:16:43.474428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total type of text: 4796519\n",
      "Max Length of entry is 249631, text is ,\n"
     ]
    }
   ],
   "source": [
    "model_type = args.model_type.lower()\n",
    "output_dir = os.path.join(args.output_dir, model_type)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "entity_linking.get_stat_inverted_index(args.index_ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T09:19:32.984356Z",
     "start_time": "2019-08-12T09:18:15.376175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source : BuboQA/entity_detection/nn/query_text/lstm/query.amt_test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21687it [01:17, 279.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amt_test\n",
      "Top1 Entity Linking Accuracy: 0.6616406141928344\n",
      "Top3 Entity Linking Accuracy: 0.7760409461889611\n",
      "Top5 Entity Linking Accuracy: 0.8102088808963895\n",
      "Top10 Entity Linking Accuracy: 0.8484806566145617\n",
      "Top20 Entity Linking Accuracy: 0.8759625582145986\n",
      "Top50 Entity Linking Accuracy: 0.9026144694978558\n",
      "Top100 Entity Linking Accuracy: 0.9197214921381472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "entity_linking.entity_linking(\"amt_test\",\n",
    "                    os.path.join(args.query_dir, \"query.amt_test\"),\n",
    "                    os.path.join(args.data_dir, \"amt_test.txt\"),\n",
    "                    args.hits,\n",
    "                    os.path.join(output_dir, \"test-h{}.txt\".format(args.hits)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T14:22:06.532594Z",
     "start_time": "2019-08-07T14:22:06.530036Z"
    }
   },
   "source": [
    "#### Running the relation module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:27:03.158134Z",
     "start_time": "2019-08-12T10:27:03.146931Z"
    }
   },
   "outputs": [],
   "source": [
    "def rel_predict(dataset_iter=test_iter, dataset=test, data_name=\"test\", result_path = './BuboQA/result'):\n",
    "    print(\"Dataset: {}\".format(data_name))\n",
    "    rel_model.eval()\n",
    "    dataset_iter.init_epoch()\n",
    "\n",
    "    n_correct = 0\n",
    "    fname = \"{}.txt\".format(data_name)\n",
    "    results_file = open(os.path.join(results_path, fname), 'w')\n",
    "    n_retrieved = 0\n",
    "\n",
    "    fid = open(os.path.join(args.data_dir,\"lineids_{}.txt\".format(data_name)))\n",
    "    sent_id = [x.strip() for x in fid.readlines()]\n",
    "\n",
    "    for data_batch_idx, data_batch in enumerate(dataset_iter):\n",
    "        scores = rel_model(data_batch)\n",
    "        if args.rel_dataset == 'RelationPrediction':\n",
    "            n_correct += torch.sum(torch.max(scores, 1)[1].view(data_batch.relation.size()).data == data_batch.relation.data).item()\n",
    "            # Get top k\n",
    "            top_k_scores, top_k_indices = torch.topk(scores, k=args.hits, dim=1, sorted=True)  # shape: (batch_size, k)\n",
    "            top_k_scores_array = top_k_scores.cpu().data.numpy()\n",
    "            top_k_indices_array = top_k_indices.cpu().data.numpy()\n",
    "            top_k_relatons_array = index2tag[top_k_indices_array]\n",
    "            for i, (relations_row, scores_row) in enumerate(zip(top_k_relatons_array, top_k_scores_array)):\n",
    "                index = (data_batch_idx * args.batch_size) + i\n",
    "                example = data_batch.dataset.examples[index]\n",
    "                for j, (rel, score) in enumerate(zip(relations_row, scores_row)):\n",
    "                    if (rel == example.relation):\n",
    "                        label = 1\n",
    "                        n_retrieved += 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                    results_file.write(\n",
    "                        \"{} %%%% {} %%%% {} %%%% {}\\n\".format( sent_id[index], rel, label, score))\n",
    "        else:\n",
    "            print(\"Wrong Dataset\")\n",
    "            exit()\n",
    "    \n",
    "    \n",
    "    if args.rel_dataset == 'RelationPrediction':\n",
    "        P = 1. * n_correct / len(dataset)\n",
    "        print(\"{} Precision: {:10.6f}%\".format(data_name, 100. * P))\n",
    "        print(\"no. retrieved: {} out of {}\".format(n_retrieved, len(dataset)))\n",
    "        retrieval_rate = 100. * n_retrieved / len(dataset)\n",
    "        print(\"{} Retrieval Rate {:10.6f}\".format(data_name, retrieval_rate))\n",
    "    else:\n",
    "        print(\"Wrong dataset\")\n",
    "        print(args.rel_dataset)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:27:03.688929Z",
     "start_time": "2019-08-12T10:27:03.681886Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RelationPrediction'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.rel_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:27:08.898143Z",
     "start_time": "2019-08-12T10:27:04.168034Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train, dev, test = relSQdataset.splits(TEXT, RELATION, path=args.data_dir, test='amt_test.txt' )\n",
    "RELATION.build_vocab(train, dev)\n",
    "index2tag = np.array(RELATION.vocab.itos)\n",
    "\n",
    "train_iter = data.Iterator(train, batch_size=args.batch_size, device=\"cuda\", train=True, repeat=False,\n",
    "                                   sort=False, shuffle=True)\n",
    "dev_iter = data.Iterator(dev, batch_size=args.batch_size, device=\"cuda\", train=False, repeat=False,\n",
    "                                   sort=False, shuffle=False)\n",
    "test_iter = data.Iterator(test, batch_size=args.batch_size, device=\"cuda\", train=False, repeat=False,\n",
    "                                   sort=False, shuffle=False)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:27:30.409516Z",
     "start_time": "2019-08-12T10:27:08.903950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: amt_test\n",
      "amt_test Precision:  82.122931%\n",
      "no. retrieved: 21390 out of 21687\n",
      "amt_test Retrieval Rate  98.630516\n"
     ]
    }
   ],
   "source": [
    "rel_predict(dataset_iter=test_iter, dataset=test, data_name=\"amt_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evidence integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:40:27.141616Z",
     "start_time": "2019-08-12T10:39:20.068185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index map from BuboQA/indexes/reachability_2M.pkl\n",
      "Loading index map from BuboQA/indexes/degrees_2M.pkl\n",
      "Loading Wiki\n"
     ]
    }
   ],
   "source": [
    "ent_type = args.ent_type.lower()\n",
    "rel_type = args.rel_type.lower()\n",
    "output_dir = os.path.join(args.output_dir, \"{}-{}\".format(ent_type, rel_type))\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "index_reach = ei.load_index(args.index_reachpath)\n",
    "index_degrees = ei.load_index(args.index_degreespath)\n",
    "mid2wiki = ei.get_mid2wiki(args.wiki_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:46:27.970638Z",
     "start_time": "2019-08-12T10:46:27.927400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load predicted MIDs and relations for each question in valid/test set\n",
    "def get_mids(filename, hits):\n",
    "    print(\"Entity Source : {}\".format(filename))\n",
    "    id2mids = defaultdict(list)\n",
    "    fin = open(filename)\n",
    "    for line in fin.readlines():\n",
    "        items = line.strip().split(' %%%% ')\n",
    "        lineid = items[0]\n",
    "        cand_mids = items[1:][:hits]\n",
    "        for mid_entry in cand_mids:\n",
    "            mid, mid_name, mid_type, score = mid_entry.split('\\t')\n",
    "            id2mids[lineid].append((mid, mid_name, mid_type, float(score)))\n",
    "    return id2mids\n",
    "\n",
    "def get_rels(filename, hits):\n",
    "    print(\"Relation Source : {}\".format(filename))\n",
    "    id2rels = defaultdict(list)\n",
    "    fin = open(filename)\n",
    "    for line in fin.readlines():\n",
    "        items = line.strip().split(' %%%% ')\n",
    "        lineid = items[0].strip()\n",
    "        rel = www2fb(items[1].strip())\n",
    "        label = items[2].strip()\n",
    "        score = items[3].strip()\n",
    "        if len(id2rels[lineid]) < hits:\n",
    "            id2rels[lineid].append((rel, label, float(score)))\n",
    "    return id2rels\n",
    "\n",
    "\n",
    "def get_questions(filename):\n",
    "    print(\"getting questions ...\")\n",
    "    id2questions = {}\n",
    "    id2goldmids = {}\n",
    "    fin =open(filename)\n",
    "    for line in fin.readlines():\n",
    "        items = line.strip().split('\\t')\n",
    "        lineid = items[0].strip()\n",
    "        mid = items[1].strip()\n",
    "        question = items[5].strip()\n",
    "        rel = items[3].strip()\n",
    "        id2questions[lineid] = (question, rel)\n",
    "        id2goldmids[lineid] = mid\n",
    "    return id2questions, id2goldmids\n",
    "\n",
    "def get_mid2wiki(filename):\n",
    "    print(\"Loading Wiki\")\n",
    "    mid2wiki = defaultdict(bool)\n",
    "    fin = open(filename)\n",
    "    for line in fin.readlines():\n",
    "        items = line.strip().split('\\t')\n",
    "        sub = rdf2fb(clean_uri(items[0]))\n",
    "        mid2wiki[sub] = True\n",
    "    return mid2wiki\n",
    "\n",
    "def evidence_integration(data_path, ent_path, rel_path, output_dir, index_reach, index_degrees, mid2wiki, is_heuristics, HITS_ENT, HITS_REL):\n",
    "    id2questions, id2goldmids = get_questions(data_path)\n",
    "    id2mids = get_mids(ent_path, HITS_ENT)\n",
    "    id2rels = get_rels(rel_path, HITS_REL)\n",
    "    file_base_name = os.path.basename(data_path)\n",
    "    fout = open(os.path.join(output_dir, file_base_name), 'w')\n",
    "\n",
    "    id2answers = defaultdict(list)\n",
    "    found, notfound_both, notfound_mid, notfound_rel = 0, 0, 0, 0\n",
    "    retrieved, retrieved_top1, retrieved_top2, retrieved_top3 = 0, 0, 0, 0\n",
    "    lineids_found1 = []\n",
    "    lineids_found2 = []\n",
    "    lineids_found3 = []\n",
    "\n",
    "    # for every lineid\n",
    "    for line_id in id2goldmids:\n",
    "        if line_id not in id2mids and line_id not in id2rels:\n",
    "            notfound_both += 1\n",
    "            continue\n",
    "        elif line_id not in id2mids:\n",
    "            notfound_mid += 1\n",
    "            continue\n",
    "        elif line_id not in id2rels:\n",
    "            notfound_rel += 1\n",
    "            continue\n",
    "\n",
    "        found += 1\n",
    "        question, truth_rel = id2questions[line_id]\n",
    "        truth_rel = www2fb(truth_rel)\n",
    "        truth_mid = id2goldmids[line_id]\n",
    "        mids = id2mids[line_id]\n",
    "        rels = id2rels[line_id]\n",
    "        if is_heuristics:\n",
    "            for (mid, mid_name, mid_type, mid_score) in mids:\n",
    "                for (rel, rel_label, rel_log_score) in rels:\n",
    "                    # if this (mid, rel) exists in FB\n",
    "                    if rel in index_reach[mid]:\n",
    "                        rel_score = math.exp(float(rel_log_score))\n",
    "                        comb_score = (float(mid_score)**0.6) * (rel_score**0.1)\n",
    "                        id2answers[line_id].append((mid, rel, mid_name, mid_type, mid_score, rel_score, comb_score, int(mid2wiki[mid]), int(index_degrees[mid][0])))\n",
    "                    # I cannot use retrieved here because I use contain different name_type\n",
    "                    # if mid ==truth_mid and rel == truth_rel:\n",
    "                    #     retrieved += 1\n",
    "            id2answers[line_id].sort(key=lambda t: (t[6], t[3],  t[7], t[8]), reverse=True)\n",
    "        else:\n",
    "            id2answers[line_id] = [(mids[0][0], rels[0][0])]\n",
    "\n",
    "        # write to file\n",
    "        fout.write(\"{}\".format(line_id))\n",
    "        for answer in id2answers[line_id]:\n",
    "            mid, rel, mid_name, mid_type, mid_score, rel_score, comb_score, _, _ = answer\n",
    "            fout.write(\" %%%% {}\\t{}\\t{}\\t{}\\t{}\".format(mid, rel, mid_name, mid_score, rel_score, comb_score))\n",
    "        fout.write('\\n')\n",
    "\n",
    "        if len(id2answers[line_id]) >= 1 and id2answers[line_id][0][0] == truth_mid \\\n",
    "                and id2answers[line_id][0][1] == truth_rel:\n",
    "            retrieved_top1 += 1\n",
    "            retrieved_top2 += 1\n",
    "            retrieved_top3 += 1\n",
    "            lineids_found1.append(line_id)\n",
    "        elif len(id2answers[line_id]) >= 2 and id2answers[line_id][1][0] == truth_mid \\\n",
    "                and id2answers[line_id][1][1] == truth_rel:\n",
    "            retrieved_top2 += 1\n",
    "            retrieved_top3 += 1\n",
    "            lineids_found2.append(line_id)\n",
    "        elif len(id2answers[line_id]) >= 3 and id2answers[line_id][2][0] == truth_mid \\\n",
    "                and id2answers[line_id][2][1] == truth_rel:\n",
    "            retrieved_top3 += 1\n",
    "            lineids_found3.append(line_id)\n",
    "\n",
    "    print()\n",
    "    print(\"found:              {}\".format(found / len(id2goldmids) * 100.0))\n",
    "    print(\"retrieved at top 1: {}\".format(retrieved_top1 / len(id2goldmids) * 100.0))\n",
    "    print(\"retrieved at top 2: {}\".format(retrieved_top2 / len(id2goldmids) * 100.0))\n",
    "    print(\"retrieved at top 3: {}\".format(retrieved_top3 / len(id2goldmids) * 100.0))\n",
    "    #print(\"retrieved at inf:   {}\".format(retrieved / len(id2goldmids) * 100.0))\n",
    "    fout.close()\n",
    "    return id2answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-12T10:49:54.927610Z",
     "start_time": "2019-08-12T10:49:44.164429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting questions ...\n",
      "Entity Source : BuboQA/entity_linking/results/lstm/test-h100.txt\n",
      "Relation Source : BuboQA/relation_prediction/nn/results/cnn/test.txt\n",
      "\n",
      "found:              99.5112279245631\n",
      "retrieved at top 1: 74.68529533822105\n",
      "retrieved at top 2: 80.91483377138377\n",
      "retrieved at top 3: 82.73620141098354\n"
     ]
    }
   ],
   "source": [
    "test_answers = ei.evidence_integration(args.data_path, args.ent_path, args.rel_path, output_dir, index_reach, index_degrees, mid2wiki, args.heuristics, args.hits_ent, args.hits_rel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
